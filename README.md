# UC Catalog Scraper & Data Extractor

Herramienta completa para descargar pÃ¡ginas del catÃ¡logo UC como PDFs y extraer informaciÃ³n estructurada, con especial Ã©nfasis en bibliografÃ­a acadÃ©mica.

## ğŸš€ CaracterÃ­sticas

### ğŸ“¥ Scraper Web
- âœ… Extrae automÃ¡ticamente todos los enlaces del catÃ¡logo UC
- âœ… Convierte pÃ¡ginas web a PDF de alta calidad
- âœ… Nombres de archivo inteligentes basados en cÃ³digos de curso
- âœ… Manejo robusto de errores y reintentos
- âœ… Evita descargas duplicadas

### ğŸ“Š Extractor de Datos
- âœ… Extrae metadatos estructurados de los PDFs
- âœ… **Enfoque especial en bibliografÃ­a** (mÃ­nima y complementaria)
- âœ… Parser inteligente para autores, tÃ­tulos, aÃ±os y URLs
- âœ… ExportaciÃ³n mÃºltiple: JSON, CSV y reportes detallados
- âœ… ValidaciÃ³n y limpieza automÃ¡tica de datos

### ğŸ”§ CaracterÃ­sticas TÃ©cnicas
- âœ… Logging detallado y reportes de ejecuciÃ³n
- âœ… ConfiguraciÃ³n centralizada y personalizable
- âœ… Arquitectura modular y escalable

## ğŸ“ Estructura del proyecto

```
uc-catalog-scraper/
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ requirements.txt             # Dependencias de Python
â”œâ”€â”€ setup.ps1                    # Script de instalaciÃ³n principal
â”œâ”€â”€ setup-extraction.ps1         # Setup adicional para extracciÃ³n
â”œâ”€â”€ run.ps1                      # Script de ejecuciÃ³n del scraper
â”œâ”€â”€ demo_extraction.py           # Demo del extractor
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py              # Scraper principal
â”‚   â”œâ”€â”€ extract_courses.py      # Extractor principal
â”‚   â”œâ”€â”€ config.py               # Configuraciones del scraper
â”‚   â”œâ”€â”€ utils.py                # Funciones auxiliares
â”‚   â””â”€â”€ pdf_extractor/          # MÃ³dulo de extracciÃ³n
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ course_extractor.py # Extractor principal
â”‚       â”œâ”€â”€ models.py           # Modelos de datos
â”‚       â”œâ”€â”€ parsers.py          # Parsers especializados
â”‚       â””â”€â”€ exporters.py        # Exportadores JSON/CSV
â”‚
â”œâ”€â”€ output/                     # PDFs descargados
â”œâ”€â”€ data/                       # Datos extraÃ­dos
â”‚   â”œâ”€â”€ extracted/              # Archivos de salida
â”‚   â””â”€â”€ reports/                # Reportes de extracciÃ³n
â””â”€â”€ logs/                       # Logs de ejecuciÃ³n
```

## ğŸ› ï¸ InstalaciÃ³n

### InstalaciÃ³n completa (Recomendado)

**1. Setup inicial del scraper:**
```powershell
.\setup.ps1
```

**2. Setup adicional para extracciÃ³n de datos:**
```powershell
.\setup-extraction.ps1
```

### InstalaciÃ³n manual

**1. Crear entorno virtual:**
```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

**2. Instalar dependencias:**
```powershell
pip install -r requirements.txt
playwright install chromium
```

## ğŸƒâ€â™‚ï¸ Uso

### 1. Descargar PDFs del catÃ¡logo

```powershell
# Ejecutar scraper
.\run.ps1

# O manualmente
python src/scraper.py
```

### 2. Extraer informaciÃ³n de los PDFs

```powershell
# Demo con un PDF
python demo_extraction.py

# Procesar todos los PDFs
python src/extract_courses.py

# Con logging detallado
python src/extract_courses.py --debug

# Personalizar directorios
python src/extract_courses.py --input-dir mi_carpeta --output-dir resultados
```

## ğŸ“Š Datos ExtraÃ­dos

### ğŸ¯ InformaciÃ³n Principal
- **Metadatos del curso**: cÃ³digo, nombre, crÃ©ditos, tipo, disciplina
- **Contenido acadÃ©mico**: descripciÃ³n, resultados de aprendizaje
- **MetodologÃ­a y evaluaciÃ³n**: estrategias y porcentajes
- **ğŸ“š BibliografÃ­a detallada**: mÃ­nima y complementaria

### ğŸ“ˆ Formatos de Salida

**JSON Completo** (`cursos_completo_YYYYMMDD_HHMMSS.json`):
```json
{
  "EPG4005": {
    "metadata": {
      "codigo": "EPG4005",
      "nombre": "METODOS BAYESIANOS",
      "creditos": 5,
      "disciplina": "ESTADISTICA"
    },
    "bibliografia": {
      "minima": [
        {
          "raw_text": "Andrew Gelman, John B. Carlin...",
          "authors": ["Andrew Gelman", "John B. Carlin"],
          "title": "Bayesian Data Analysis",
          "year": 2013,
          "publisher": "CRC Press"
        }
      ]
    }
  }
}
```

**JSON de BibliografÃ­a** (`bibliografia_YYYYMMDD_HHMMSS.json`):
```json
{
  "EPG4005": {
    "codigo": "EPG4005",
    "nombre": "METODOS BAYESIANOS",
    "bibliografia": {...},
    "total_entradas": 5
  }
}
```

**CSV de Metadatos** (`metadatos_YYYYMMDD_HHMMSS.csv`):
```csv
codigo,nombre,creditos,disciplina,has_bibliography,total_bib_entries
EPG4005,METODOS BAYESIANOS,5,ESTADISTICA,true,5
```

**CSV de BibliografÃ­a** (`bibliografia_YYYYMMDD_HHMMSS.csv`):
```csv
curso_codigo,tipo_bibliografia,authors,title,year,publisher
EPG4005,minima,"Andrew Gelman; John B. Carlin",Bayesian Data Analysis,2013,CRC Press
```

## âš™ï¸ ConfiguraciÃ³n

### ConfiguraciÃ³n del Scraper (`src/config.py`)
```python
SCRAPER_CONFIG = {
    "delay_between_requests": 2,  # segundos entre descargas
    "request_timeout": 30,        # timeout de requests
    "max_retries": 3,            # reintentos por pÃ¡gina
}

PDF_CONFIG = {
    "format": "A4",
    "margin": {"top": "0.75in", "right": "0.75in", "bottom": "0.75in", "left": "0.75in"}
}
```

### ConfiguraciÃ³n del Extractor
```powershell
# Solo JSON
python src/extract_courses.py --export-format json

# Solo CSV  
python src/extract_courses.py --export-format csv

# Ambos (default)
python src/extract_courses.py --export-format both
```

## ğŸ“ˆ Casos de Uso

### ğŸ” InvestigaciÃ³n BibliogrÃ¡fica
- **AnÃ¡lisis de tendencias**: Autores y obras mÃ¡s citadas
- **Mapeo disciplinario**: BibliografÃ­a por Ã¡rea de conocimiento
- **EvoluciÃ³n temporal**: Cambios en referencias a lo largo del tiempo

### ğŸ“Š AnÃ¡lisis Curricular
- **DistribuciÃ³n de crÃ©ditos** por facultad y disciplina
- **Tipos de evaluaciÃ³n** mÃ¡s comunes
- **Palabras clave** y tendencias temÃ¡ticas
- **Carga acadÃ©mica** y balanceo curricular

### ğŸ¯ BÃºsquedas Inteligentes
```python
# Ejemplos de consultas posibles con los datos extraÃ­dos
"Cursos de 5 crÃ©ditos con laboratorio"
"BibliografÃ­a de IA y Machine Learning"
"Cursos sin prerrequisitos por facultad"
"EvoluciÃ³n de metodologÃ­as de evaluaciÃ³n"
```

## ğŸ”§ SoluciÃ³n de problemas

### Problemas del Scraper
- **"Playwright no estÃ¡ instalado"**: `pip install playwright && playwright install chromium`
- **"No se encontraron enlaces"**: Verificar conexiÃ³n y estructura de la pÃ¡gina UC
- **PDFs vacÃ­os**: Aumentar `wait_for_load` en config.py

### Problemas del Extractor
- **"Error importando mÃ³dulos"**: Ejecutar `setup-extraction.ps1`
- **"No se encontraron PDFs"**: Asegurar que `/output` contiene PDFs
- **"BibliografÃ­a incompleta"**: El parser maneja variaciones de formato automÃ¡ticamente

### Problemas de Rendimiento
- **Memoria**: El procesamiento es secuencial para optimizar uso de memoria
- **Velocidad**: Ajustar `delay_between_requests` segÃºn capacidad del servidor UC

## ğŸ“Š Ejemplo de Resultados

```
=== RESULTADOS DE EXTRACCIÃ“N ===
Archivos procesados: 47
Extracciones exitosas: 45
Tasa de Ã©xito: 95.7%
Cursos con bibliografÃ­a: 42
Cobertura bibliogrÃ¡fica: 89.4%
Total entradas bibliogrÃ¡ficas: 312

Top 5 cursos con mÃ¡s bibliografÃ­a:
  1. EPG4005 - METODOS BAYESIANOS: 8 entradas
  2. FIL2000 - ETICA APLICADA A INTELIGENCIA ARTIFICIAL: 7 entradas
  3. IMT3870 - COMPUTACION DE ALTO RENDIMIENTO: 6 entradas
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/DataExtraction`)
3. Commit tus cambios (`git commit -m 'Add PDF data extraction'`)
4. Push a la rama (`git push origin feature/DataExtraction`)
5. Abre un Pull Request

## ğŸ“ Notas TÃ©cnicas

### Scraper
- Respeta servidores UC con pausas entre requests
- Evita descargas duplicadas automÃ¡ticamente
- Usa Playwright para renderizado completo de JavaScript

### Extractor
- Parser robusto para formatos variados de bibliografÃ­a
- ValidaciÃ³n automÃ¡tica de datos extraÃ­dos
- Manejo inteligente de caracteres especiales y encoding