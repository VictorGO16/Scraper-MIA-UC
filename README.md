# UC Catalog Scraper

Herramienta para descargar automÃ¡ticamente todas las pÃ¡ginas del catÃ¡logo UC como archivos PDF.

## ğŸš€ CaracterÃ­sticas

- âœ… Extrae automÃ¡ticamente todos los enlaces del catÃ¡logo UC
- âœ… Convierte pÃ¡ginas web a PDF de alta calidad
- âœ… Nombres de archivo inteligentes basados en cÃ³digos de curso
- âœ… Manejo robusto de errores y reintentos
- âœ… Logging detallado y reportes de ejecuciÃ³n
- âœ… Evita descargas duplicadas
- âœ… ConfiguraciÃ³n centralizada y personalizable

## ğŸ“ Estructura del proyecto

```
uc-catalog-scraper/
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ requirements.txt       # Dependencias de Python
â”œâ”€â”€ setup.ps1              # Scripts de instalaciÃ³n
â”œâ”€â”€ run.ps1                # Scripts de ejecuciÃ³n
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py        # CÃ³digo principal
â”‚   â”œâ”€â”€ config.py         # Configuraciones
â”‚   â””â”€â”€ utils.py          # Funciones auxiliares
â”œâ”€â”€ output/               # PDFs generados
â””â”€â”€ logs/                 # Logs de ejecuciÃ³n
```

## ğŸ› ï¸ InstalaciÃ³n

### MÃ©todo 1: Usando scripts de instalaciÃ³n (Recomendado)

**Windows:**
```powershell
.\setup.ps1
```

### MÃ©todo 2: InstalaciÃ³n manual

1. **Crear entorno virtual:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
# o
venv\Scripts\activate.bat  # Windows
```

2. **Instalar dependencias:**
```bash
pip install -r requirements.txt
playwright install chromium
```

## ğŸƒâ€â™‚ï¸ Uso

### EjecuciÃ³n simple

**Windows:**
```powershell
.\run.ps1
```

### EjecuciÃ³n manual
```bash
# Activar entorno virtual
source venv/bin/activate  # Linux/macOS
# o
venv\Scripts\activate.bat  # Windows

# Ejecutar scraper
python src/scraper.py
```

## âš™ï¸ ConfiguraciÃ³n

Puedes personalizar el comportamiento editando `src/config.py`:

```python
# ConfiguraciÃ³n del scraper
SCRAPER_CONFIG = {
    "delay_between_requests": 2,  # segundos entre descargas
    "request_timeout": 30,        # timeout de requests
    "max_retries": 3,            # reintentos por pÃ¡gina
}

# ConfiguraciÃ³n de PDF
PDF_CONFIG = {
    "format": "A4",              # tamaÃ±o de pÃ¡gina
    "margin": {                  # mÃ¡rgenes
        "top": "0.75in",
        "right": "0.75in",
        "bottom": "0.75in",
        "left": "0.75in"
    }
}
```

## ğŸ“Š Salida

### Archivos generados:
- **`output/`**: Contiene todos los PDFs descargados
  - Nombres basados en cÃ³digos de curso (ej: `EPG4007.pdf`)
  - Fallback a hash para pÃ¡ginas sin cÃ³digo
  
- **`logs/`**: Contiene logs de ejecuciÃ³n
  - `scraper.log`: Log detallado de cada ejecuciÃ³n
  - `ultimo_reporte.txt`: Resumen de la Ãºltima ejecuciÃ³n

### Ejemplo de salida:
```
=== REPORTE DE EJECUCIÃ“N ===
Descargas exitosas: 45
Descargas fallidas: 2
Total de enlaces: 47
Tasa de Ã©xito: 95.7%

Archivos generados:
  - EPG4007.pdf (234.5 KB)
  - EPG4008.pdf (198.2 KB)
  - EPG4009.pdf (267.8 KB)
  ...
```

## ğŸ”§ SoluciÃ³n de problemas

### Error: "Playwright no estÃ¡ instalado"
```bash
pip install playwright
playwright install chromium
```

### Error: "No se encontraron enlaces"
- Verifica tu conexiÃ³n a internet
- La pÃ¡gina de la UC podrÃ­a haber cambiado su estructura
- Revisa los logs en `logs/scraper.log`

### PDFs vacÃ­os o incorrectos
- Algunas pÃ¡ginas pueden requerir JavaScript para cargar
- El scraper espera 3 segundos adicionales, pero puedes aumentar `wait_for_load` en config.py

### Problemas de memoria
- Si tienes muchas pÃ¡ginas, el scraper procesa una por vez para evitar problemas de memoria
- Puedes ajustar `delay_between_requests` para reducir la carga

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Notas

- El scraper respeta los servidores de la UC con pausas entre requests
- Los archivos existentes no se re-descargan (evita duplicados)
- Todas las URLs se validan antes de procesarse
- El proyecto usa logging profesional para debugging